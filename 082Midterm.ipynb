{
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_LNprMxhMKMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4p1lmAq7yLOb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "import random\n",
        "import zlib\n",
        "from sklearn.neighbors import KNeighborsClassifier as knn\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_random_dataset(n_samples, d):\n",
        "    X = np.random.rand(n_samples, d)  # Random features between 0 and 1\n",
        "    y = np.random.randint(2, size=n_samples)  # Random binary labels (0 or 1)\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "pKFMB4jNGUW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-EfL3Pg0JhT"
      },
      "source": [
        "# Question 6.1 a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbIkNcJd0M-E"
      },
      "outputs": [],
      "source": [
        "#number of points have to memorize is average over all points that need to be kept with 1-NN\n",
        "#n_full/n_average approaches 2 as d increases based on counting function theorem, so that n_full cn be reduced to n_full/2bits to prove efficient generalization and compression\n",
        "\n",
        "#n_full = 2**D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynYu8F2HsQAz"
      },
      "outputs": [],
      "source": [
        "def condensed_dataset_knn(X, y):\n",
        "    C_X = np.array([X[0]])\n",
        "    C_y = np.array([y[0]])\n",
        "    KNN = knn(n_neighbors=1)\n",
        "\n",
        "    for i in range(1, len(X)):\n",
        "        KNN.fit(C_X, C_y)\n",
        "        if KNN.predict([X[i]])[0] != y[i]:\n",
        "            C_X = np.append(C_X, [X[i]], axis=0)\n",
        "            C_y = np.append(C_y, [y[i]])\n",
        "\n",
        "    return C_X, C_y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aj7LjV6Gsoxq",
        "outputId": "caef65cf-a429-40bd-9c7c-e59493d033da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "d=2: n_full=4, Avg. req. points for memorization n_avg=2.19, n_full/n_avg=1.8285714285714285\n",
            "d=4: n_full=16, Avg. req. points for memorization n_avg=8.50, n_full/n_avg=1.8823529411764706\n",
            "d=8: n_full=256, Avg. req. points for memorization n_avg=129.39, n_full/n_avg=1.9785050114720444\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(42)\n",
        "dimensionalities = [2, 4, 8]\n",
        "functions_per_dimension = {2: 16, 4: 32, 8: 64}\n",
        "\n",
        "for d, num_functions in functions_per_dimension.items():\n",
        "    n_full = 2 ** d  # Total possible configurations\n",
        "    memorization_counts = []\n",
        "\n",
        "    for _ in range(num_functions):\n",
        "        X, y = generate_random_dataset(n_full, d)  # Generate dataset for each function\n",
        "        _, condensed_y = condensed_dataset_knn(X, y)  # Apply CNN\n",
        "        memorization_counts.append(len(condensed_y))  # Record the number of points kept\n",
        "    n_avg = np.mean(memorization_counts)\n",
        "    print(f\"d={d}: n_full={2**d}, Avg. req. points for memorization n_avg={n_avg:.2f}, n_full/n_avg={(2**d)/n_avg}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vaEsAKhRuIT"
      },
      "source": [
        "# Question 6.1 b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckY9j2z1Rwqh"
      },
      "outputs": [],
      "source": [
        "def generate_random_dataset_multi(n_samples, n_features, n_classes=3):\n",
        "    X = np.random.rand(n_samples, n_features)  # Random features\n",
        "    y = np.random.randint(0, n_classes, size=n_samples)  # Random labels for multiple classes\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pkb7sJxnR8Si",
        "outputId": "c9f5db90-2cbd-4344-ecfc-6ba2cf15261b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "d=2: n_full=9, number of classes=3, Avg. req. points for memorization n_avg=6.26, n_full/n_avg=1.4378698224852071\n",
            "d=4: n_full=81, number of classes=3, Avg. req. points for memorization n_avg=55.28, n_full/n_avg=1.4653266331658292\n",
            "d=8: n_full=6561, number of classes=3, Avg. req. points for memorization n_avg=4373.92, n_full/n_avg=1.5000285785051535\n"
          ]
        }
      ],
      "source": [
        "dimensionalities = [2, 4, 8]\n",
        "n_classes = 3\n",
        "functions_per_dim = {2: 27, 4: 54, 8: 108}\n",
        "\n",
        "for d in dimensionalities:\n",
        "    all_memorization_sizes = []\n",
        "\n",
        "    for func in range(functions_per_dim[d]):\n",
        "        X, y = generate_random_dataset_multi(3**d, d, n_classes)\n",
        "        # Apply the condensed KNN algorithm\n",
        "        condensed_X, condensed_y = condensed_dataset_knn(X, y)\n",
        "        # Store the size of the condensed dataset\n",
        "        all_memorization_sizes.append(len(condensed_y))\n",
        "\n",
        "    avg_mem_size = np.mean(all_memorization_sizes)\n",
        "    print(f\"d={d}: n_full={3**d}, number of classes={n_classes}, Avg. req. points for memorization n_avg={avg_mem_size:.2f}, n_full/n_avg={(3**d)/avg_mem_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 6.2 a"
      ],
      "metadata": {
        "id": "TKYdpVt204-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier, export_text\n",
        "data = load_breast_cancer()"
      ],
      "metadata": {
        "id": "URtndUvo1BAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = data.data\n",
        "y = data.target  # Convert to binary classification problem\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "clf_OG = DecisionTreeClassifier(max_depth=None,  # No maximum depth to allow more complexity\n",
        "                             min_samples_split=2,  # Minimum samples required to split a node\n",
        "                             min_samples_leaf=1,  # Minimum samples required to be a leaf node\n",
        "                             random_state=42)\n",
        "\n",
        "clf_OG.fit(X_train, y_train)\n",
        "\n",
        "def clauses(data, clf):\n",
        "  tree_rules = export_text(clf, feature_names=list(data.feature_names))\n",
        "  predictions = clf.predict(X_test)\n",
        "  accuracy = accuracy_score(y_test, predictions)\n",
        "  print(f\"Accuracy on the test set: {accuracy:.2f}\")\n",
        "  return tree_rules\n",
        "\n",
        "def num_clauses(tree_rules):\n",
        "  if_then_clauses = [line for line in tree_rules.split('\\n') if \"class:\" in line]\n",
        "  number_of_clauses = len(if_then_clauses)\n",
        "  print(f\"Number of if-then clauses: {number_of_clauses}\")\n",
        "\n",
        "tree_rules = clauses(data, clf_OG)\n",
        "max_depth = clf_OG.tree_.max_depth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnQzFGsb1h9o",
        "outputId": "c6f6986a-e584-48ff-b0c7-442d3c20201a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the test set: 0.94\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_clauses(tree_rules)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Zhu_CHYe86p",
        "outputId": "b70827c4-2d86-45ef-d9ca-f1cf73d0882a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of if-then clauses: 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Increasing min-samples split and min_samples_leaf"
      ],
      "metadata": {
        "id": "8ikim71fgryb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf_min = DecisionTreeClassifier(max_depth=None,\n",
        "                             min_samples_split=8,\n",
        "                             min_samples_leaf=8,\n",
        "                             random_state=42)\n",
        "\n",
        "clf_min.fit(X_train, y_train)\n",
        "tree_rules = clauses(data, clf_min)\n",
        "num_clauses(tree_rules)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGubakXpguHG",
        "outputId": "6c8d9a4b-ac17-4297-c30d-6dbe98b3a6bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the test set: 0.94\n",
            "Number of if-then clauses: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decreasing max_depth"
      ],
      "metadata": {
        "id": "XD7gbJnbhz70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf_depth = DecisionTreeClassifier(max_depth=int(max_depth/3),\n",
        "                             min_samples_split=2,\n",
        "                             min_samples_leaf=1,\n",
        "                             random_state=42)\n",
        "\n",
        "clf_depth.fit(X_train, y_train)\n",
        "tree_rules = clauses(data, clf_depth)\n",
        "num_clauses(tree_rules)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pr4n7FwiEeq",
        "outputId": "f2f91bee-3cdd-4c24-d6ae-e78d5c76280d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the test set: 0.93\n",
            "Number of if-then clauses: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 6.2 b"
      ],
      "metadata": {
        "id": "nUANFk5Oi4mD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Different Dataset #1"
      ],
      "metadata": {
        "id": "fM2azTtpU68F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ucimlrepo\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADDUG814Pexj",
        "outputId": "950307ac-18da-4a48-fc8d-3d2f1e6fbd90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ucimlrepo in /usr/local/lib/python3.10/dist-packages (0.0.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ucimlrepo import fetch_ucirepo\n",
        "banknote_authentication = fetch_ucirepo(id=267)\n",
        "X = banknote_authentication.data.features\n",
        "y = banknote_authentication.data.targets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
      ],
      "metadata": {
        "id": "XuS-GLRwPkAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf_OG.fit(X_train, y_train)\n",
        "def clauses_changed(data, clf):\n",
        "  tree_rules = export_text(clf, feature_names=list(data.features))\n",
        "  predictions = clf.predict(X_test)\n",
        "  accuracy = accuracy_score(y_test, predictions)\n",
        "  print(f\"Accuracy on the test set: {accuracy:.2f}\")\n",
        "  return tree_rules\n",
        "tree_rules = clauses_changed(banknote_authentication.data, clf_OG)\n",
        "num_clauses(tree_rules)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fLkmOAnPkmu",
        "outputId": "92c1d678-17e5-4d1c-f2e1-b8b957147930"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the test set: 0.98\n",
            "Number of if-then clauses: 27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf_min.fit(X_train, y_train)\n",
        "tree_rules = clauses_changed(banknote_authentication.data, clf_min)\n",
        "num_clauses(tree_rules)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uH71n2HfU0so",
        "outputId": "a018268c-bfc0-4273-c7c1-4ff816ffebf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the test set: 0.97\n",
            "Number of if-then clauses: 21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf_depth.fit(X_train, y_train)\n",
        "tree_rules = clauses_changed(banknote_authentication.data, clf_depth)\n",
        "num_clauses(tree_rules)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpRqMD1sUpr7",
        "outputId": "716bae93-ed65-44c4-add2-76e27ab9b35b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the test set: 0.91\n",
            "Number of if-then clauses: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Different Dataset #2"
      ],
      "metadata": {
        "id": "OcjaW36BVDMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ucimlrepo import fetch_ucirepo\n",
        "haberman_s_survival = fetch_ucirepo(id=43)\n",
        "X = haberman_s_survival.data.features\n",
        "y = haberman_s_survival.data.targets\n",
        "print(haberman_s_survival.metadata)\n",
        "print(haberman_s_survival.variables)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WE2UltxUVFT8",
        "outputId": "d0a0b78a-086c-401e-dbe9-ecb18f3757a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'uci_id': 43, 'name': \"Haberman's Survival\", 'repository_url': 'https://archive.ics.uci.edu/dataset/43/haberman+s+survival', 'data_url': 'https://archive.ics.uci.edu/static/public/43/data.csv', 'abstract': 'Dataset contains cases from study conducted on the survival of patients who had undergone surgery for breast cancer', 'area': 'Health and Medicine', 'tasks': ['Classification'], 'characteristics': ['Multivariate'], 'num_instances': 306, 'num_features': 3, 'feature_types': ['Integer'], 'demographics': ['Age'], 'target_col': ['survival_status'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 1976, 'last_updated': 'Mon Mar 04 2024', 'dataset_doi': '10.24432/C5XK51', 'creators': ['S. Haberman'], 'intro_paper': None, 'additional_info': {'summary': \"The dataset contains cases from a study that was conducted between 1958 and 1970 at the University of Chicago's Billings Hospital on the survival of patients who had undergone surgery for breast cancer.\", 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': \"   1. Age of patient at time of operation (numerical)\\r\\n   2. Patient's year of operation (year - 1900, numerical)\\r\\n   3. Number of positive axillary nodes detected (numerical)\\r\\n   4. Survival status (class attribute)\\r\\n        -- 1 = the patient survived 5 years or longer\\r\\n        -- 2 = the patient died within 5 year\", 'citation': None}}\n",
            "                       name     role     type demographic description units  \\\n",
            "0                       age  Feature  Integer         Age        None  None   \n",
            "1            operation_year  Feature  Integer        None        None  None   \n",
            "2  positive_auxillary_nodes  Feature  Integer        None        None  None   \n",
            "3           survival_status   Target  Integer        None        None  None   \n",
            "\n",
            "  missing_values  \n",
            "0             no  \n",
            "1             no  \n",
            "2             no  \n",
            "3             no  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf_OG.fit(X_train, y_train)\n",
        "tree_rules = clauses_changed(banknote_authentication.data, clf_OG)\n",
        "num_clauses(tree_rules)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vt0nstohVlCc",
        "outputId": "1e50fb05-5e28-4a84-bd14-fcbf929b931d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the test set: 0.98\n",
            "Number of if-then clauses: 27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf_min.fit(X_train, y_train)\n",
        "tree_rules = clauses_changed(banknote_authentication.data, clf_min)\n",
        "num_clauses(tree_rules)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hA8VhfyUVqmL",
        "outputId": "e992928c-94bf-446e-e309-3069416ee506"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the test set: 0.97\n",
            "Number of if-then clauses: 21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf_depth.fit(X_train, y_train)\n",
        "tree_rules = clauses_changed(banknote_authentication.data, clf_depth)\n",
        "num_clauses(tree_rules)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7zzunOdVtK-",
        "outputId": "594c5e7c-81b4-4307-e0f8-a521de27e52d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the test set: 0.91\n",
            "Number of if-then clauses: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 6.2 c"
      ],
      "metadata": {
        "id": "_RtkRcupGXnc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_decision_tree(num_samples, num_features, clf):\n",
        "    X = np.random.randint(2, size=(num_samples, num_features))\n",
        "    y = np.random.randint(2, size=(num_samples))\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "    clf.fit(X_train, y_train)\n",
        "    predictions = clf.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "\n",
        "    print(f\"Accuracy for {num_samples} samples and {num_features} features: {accuracy:.2f}\")\n",
        "classifiers = {\n",
        "    'clf_OG': clf_OG,\n",
        "    'clf_min': clf_min,\n",
        "    'clf_depth': clf_depth\n",
        "}\n",
        "# Evaluate the model with different numbers of samples and features\n",
        "for clf_name, clf in classifiers.items():\n",
        "  print(clf_name)\n",
        "  for num_samples in [100, 500, 1000]:\n",
        "        for num_features in [5, 10, 20]:\n",
        "            evaluate_decision_tree(num_samples, num_features, clf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4AfBEBJV0x7",
        "outputId": "d4ed5192-356c-4009-bd8e-a20d6d2aebd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "clf_OG\n",
            "Accuracy for 100 samples and 5 features: 0.43\n",
            "Accuracy for 100 samples and 10 features: 0.43\n",
            "Accuracy for 100 samples and 20 features: 0.53\n",
            "Accuracy for 500 samples and 5 features: 0.47\n",
            "Accuracy for 500 samples and 10 features: 0.52\n",
            "Accuracy for 500 samples and 20 features: 0.54\n",
            "Accuracy for 1000 samples and 5 features: 0.54\n",
            "Accuracy for 1000 samples and 10 features: 0.45\n",
            "Accuracy for 1000 samples and 20 features: 0.45\n",
            "clf_min\n",
            "Accuracy for 100 samples and 5 features: 0.50\n",
            "Accuracy for 100 samples and 10 features: 0.60\n",
            "Accuracy for 100 samples and 20 features: 0.33\n",
            "Accuracy for 500 samples and 5 features: 0.59\n",
            "Accuracy for 500 samples and 10 features: 0.47\n",
            "Accuracy for 500 samples and 20 features: 0.50\n",
            "Accuracy for 1000 samples and 5 features: 0.52\n",
            "Accuracy for 1000 samples and 10 features: 0.49\n",
            "Accuracy for 1000 samples and 20 features: 0.49\n",
            "clf_depth\n",
            "Accuracy for 100 samples and 5 features: 0.53\n",
            "Accuracy for 100 samples and 10 features: 0.63\n",
            "Accuracy for 100 samples and 20 features: 0.30\n",
            "Accuracy for 500 samples and 5 features: 0.47\n",
            "Accuracy for 500 samples and 10 features: 0.55\n",
            "Accuracy for 500 samples and 20 features: 0.53\n",
            "Accuracy for 1000 samples and 5 features: 0.51\n",
            "Accuracy for 1000 samples and 10 features: 0.48\n",
            "Accuracy for 1000 samples and 20 features: 0.51\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFlb1mzTHB1T"
      },
      "source": [
        "# Question 6.3 a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlj6F0GqHQf1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33640edd-dc5a-471a-d516-f7d5678231ac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7526, 0.7526)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# Generate a long random string of 10000 characters\n",
        "random_string = ''.join(random.choices('ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789', k=10000))\n",
        "\n",
        "# Compress the string using zlib\n",
        "compressed_string = zlib.compress(random_string.encode())\n",
        "\n",
        "# Calculate the compression ratio\n",
        "compression_ratio = len(compressed_string) / len(random_string)\n",
        "\n",
        "(len(compressed_string), compression_ratio)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KvWnf_XHUeT"
      },
      "source": [
        "# 6.3 b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94GKMzp5Mw1N"
      },
      "source": [
        "In the realm of lossless compression, as applied to highly random or uniformly distributed data (like the long random string we generated), the theory suggests that significant compression (i.e., a substantial reduction in size) is typically not achievable. This is because the compression ratio's theoretical best-case scenario, especially for lossless algorithms, hinges on the presence of redundancy within the data. For completely random data, which lacks any pattern or redundancy, the expected compression ratio approaches 1, meaning no effective compression is achieved.\n",
        "\n",
        "The expected compression ratio (G) in such contexts is guided by the notion that the entropy (a measure of randomness or unpredictability) in highly random data is maximal. When data is maximally random, each bit is as informative as possible, and therefore, there's little to no scope for reducing the amount of information (hence, size) without losing data integrity, which is not permissible in lossless compression.\n",
        "\n",
        "From Chapter 6, it is likely discussed that the theoretical limit for the compression ratio for a perfectly efficient compression algorithm would be bounded by the entropy per bit in the data. However, since real-world algorithms may not reach this perfect efficiency, especially for random data, we expect the compression ratio to hover around 1, as observed with the random string example.\n",
        "\n",
        "In the specific case of our experiment with the random string, the observed compression ratio was approximately 0.7528. This is slightly less than 1, indicating a small reduction from the original size, but still close to the expected value for highly random data. The slight reduction might be due to some coincidental patterns or the overhead reduction in specific parts of the data, but it's far from the significant compression seen with more redundant or patterned data.\n",
        "\n",
        "Therefore, in line with the theory from Chapter 6, the expected compression ratio for lossless compression applied to a dataset with high randomness (like our generated string) would indeed be close to 1, reflecting the minimal efficiency in compressing such data due to its lack of redundancy and high entropy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrsGLNw0d9qK"
      },
      "source": [
        "# Question 8.4 b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iG4lUDP4yO25"
      },
      "outputs": [],
      "source": [
        "def memorize(data, labels):\n",
        "    # Step 1: Pair sum of d-dimensional vectors with labels\n",
        "    table = [(np.sum(vector), label) for vector, label in zip(data, labels)]\n",
        "\n",
        "    # Step 2: Sort the table based on the sum\n",
        "    sorted_table = sorted(table, key=lambda x: x[0])\n",
        "\n",
        "    # Step 3: Determine the number of thresholds\n",
        "    thresholds = 0\n",
        "    previous_label = None\n",
        "    for _, label in sorted_table:\n",
        "        if label != previous_label:\n",
        "            thresholds += 1\n",
        "            previous_label = label\n",
        "\n",
        "    # Step 4: Calculate minimum thresholds\n",
        "    min_thresholds = np.log2(thresholds + 1)\n",
        "\n",
        "    # Step 5: Calculate MEC using the adjusted formula\n",
        "    d = data.shape[1]  # Dimensionality of the input vectors\n",
        "    mec = (min_thresholds * (d + 1)) + min_thresholds\n",
        "\n",
        "    return mec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsWTcFMMfTN7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "791c6da0-cfc8-4e66-92e4-629401267e37"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "199.32219809586817"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "points = df.iloc[:, :-1].values  # Selects all rows and all columns except the last one as data\n",
        "labels = df.iloc[:, -1].values\n",
        "\n",
        "memorize(points, labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vea5aMhZtWo-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ae4cab8-6531-476d-cd40-3968e225d6a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Memory-Equivalent Capacity: 18.575424759098897\n"
          ]
        }
      ],
      "source": [
        "def memorize_extended(data, labels):\n",
        "    overall_mec = 0\n",
        "    num_classifications = labels.shape[1]\n",
        "\n",
        "    for i in range(num_classifications):\n",
        "        current_labels = labels[:, i]\n",
        "\n",
        "        mec = memorize(data, current_labels)\n",
        "        overall_mec += mec\n",
        "\n",
        "    return overall_mec\n",
        "\n",
        "# Example usage with a hypothetical dataset and labels for multiple binary classifications\n",
        "data = np.array([\n",
        "    [1, 2],\n",
        "    [2, 3],\n",
        "    [3, 4],\n",
        "    [4, 5]\n",
        "])\n",
        "\n",
        "labels = np.array([\n",
        "    [0, 1],\n",
        "    [1, 0],\n",
        "    [0, 1],\n",
        "    [1, 0]\n",
        "])\n",
        "\n",
        "overall_mec = memorize_extended(data, labels)\n",
        "print(f\"Overall Memory-Equivalent Capacity: {overall_mec}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TrNk_dYw4bD"
      },
      "source": [
        "# Question 8.4 c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HQHWax5w4Mv"
      },
      "outputs": [],
      "source": [
        "def memorize_regression(data, labels, num_bins):\n",
        "    bins = np.linspace(min(labels), max(labels), num_bins + 1)\n",
        "    binned_labels = np.digitize(labels, bins) - 1  # Convert to bin indices\n",
        "\n",
        "    mec = memorize(data, binned_labels)\n",
        "    return mec, binned_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8WGtfb_xKC8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15cedf7b-c641-43f6-e3f3-19f75fe28805"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Approximated Memory-Equivalent Capacity for Regression: 8.0\n",
            "Binned Labels: [0 0 1 2]\n"
          ]
        }
      ],
      "source": [
        "regression_data = np.array([\n",
        "    [1, 2],\n",
        "    [2, 3],\n",
        "    [3, 4],\n",
        "    [4, 5]\n",
        "])\n",
        "regression_labels = np.array([100, 200, 300, 400])\n",
        "num_bins = 2  # Example number of bins\n",
        "mec, binned_labels = memorize_regression(regression_data, regression_labels, num_bins)\n",
        "print(f\"Approximated Memory-Equivalent Capacity for Regression: {mec}\")\n",
        "print(f\"Binned Labels: {binned_labels}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 9"
      ],
      "metadata": {
        "id": "1eGz2qhrL69y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzSlZzIOGX_m",
        "outputId": "a37ee893-4e25-4e0b-9916-3dff1df6fb0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.7)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.10.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Load the MNIST dataset\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Normalize the data\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "# Reshape images for the CNN input\n",
        "train_images = train_images.reshape((60000, 28, 28, 1))\n",
        "test_images = test_images.reshape((10000, 28, 28, 1))\n",
        "\n",
        "# Build the CNN model\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_images, train_labels, epochs=5, batch_size=64)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(f\"Test accuracy: {test_acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AhDGcsLGYrV",
        "outputId": "e905a7d8-261d-4cad-a0b3-41a64f8e40ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "938/938 [==============================] - 68s 70ms/step - loss: 0.1820 - accuracy: 0.9447\n",
            "Epoch 2/5\n",
            "938/938 [==============================] - 55s 58ms/step - loss: 0.0513 - accuracy: 0.9836\n",
            "Epoch 3/5\n",
            "938/938 [==============================] - 53s 57ms/step - loss: 0.0369 - accuracy: 0.9885\n",
            "Epoch 4/5\n",
            "938/938 [==============================] - 53s 57ms/step - loss: 0.0299 - accuracy: 0.9902\n",
            "Epoch 5/5\n",
            "938/938 [==============================] - 53s 57ms/step - loss: 0.0231 - accuracy: 0.9924\n",
            "313/313 [==============================] - 5s 14ms/step - loss: 0.0338 - accuracy: 0.9904\n",
            "Test accuracy: 0.9904000163078308\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J9zJ864VIDfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding more convolutional layers and activation functions change"
      ],
      "metadata": {
        "id": "C56HquXeIkby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_v1 = models.Sequential([\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model_v1.compile(optimizer='adam',\n",
        "                 loss='sparse_categorical_crossentropy',\n",
        "                 metrics=['accuracy'])\n",
        "model_v1.fit(train_images, train_labels, epochs=5, batch_size=64)\n",
        "test_loss, test_acc = model_v1.evaluate(test_images, test_labels)\n",
        "print(f\"Test accuracy: {test_acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRV4glpCInI-",
        "outputId": "39b20853-0767-45b0-fb4e-953688a3a379"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "938/938 [==============================] - 228s 242ms/step - loss: 0.1274 - accuracy: 0.9603\n",
            "Epoch 2/5\n",
            "938/938 [==============================] - 224s 239ms/step - loss: 0.0394 - accuracy: 0.9877\n",
            "Epoch 3/5\n",
            "938/938 [==============================] - 225s 239ms/step - loss: 0.0269 - accuracy: 0.9912\n",
            "Epoch 4/5\n",
            "938/938 [==============================] - 223s 238ms/step - loss: 0.0196 - accuracy: 0.9936\n",
            "Epoch 5/5\n",
            "938/938 [==============================] - 223s 238ms/step - loss: 0.0155 - accuracy: 0.9951\n",
            "313/313 [==============================] - 11s 36ms/step - loss: 0.0298 - accuracy: 0.9912\n",
            "Test accuracy: 0.9911999702453613\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Changing Hyperparameters"
      ],
      "metadata": {
        "id": "MfEE3z4BIpVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_v2 = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model_v2.compile(optimizer=tf.keras.optimizers.SGD(lr=0.01),\n",
        "                 loss='sparse_categorical_crossentropy',\n",
        "                 metrics=['accuracy'])\n",
        "\n",
        "model_v2.fit(train_images, train_labels, epochs=5, batch_size=64)\n",
        "test_loss, test_acc = model_v2.evaluate(test_images, test_labels)\n",
        "print(f\"Test accuracy: {test_acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrbapTteIrmc",
        "outputId": "02a790ab-c57f-4954-ad1d-d995a3dd79c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "938/938 [==============================] - 68s 72ms/step - loss: 0.9492 - accuracy: 0.7164\n",
            "Epoch 2/5\n",
            "938/938 [==============================] - 61s 65ms/step - loss: 0.2037 - accuracy: 0.9383\n",
            "Epoch 3/5\n",
            "938/938 [==============================] - 73s 77ms/step - loss: 0.1338 - accuracy: 0.9599\n",
            "Epoch 4/5\n",
            "938/938 [==============================] - 77s 82ms/step - loss: 0.1047 - accuracy: 0.9675\n",
            "Epoch 5/5\n",
            "938/938 [==============================] - 81s 86ms/step - loss: 0.0875 - accuracy: 0.9729\n",
            "313/313 [==============================] - 4s 13ms/step - loss: 0.0657 - accuracy: 0.9801\n",
            "Test accuracy: 0.9800999760627747\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Applying Measurements"
      ],
      "metadata": {
        "id": "ubUsIOLIPxDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparameter_grid = {\n",
        "    'num_layers': [2, 3, 4],  # Number of convolutional layers\n",
        "    'num_neurons': [32, 64, 128],  # Number of neurons in the dense layer\n",
        "    'batch_size': [32, 64, 128],\n",
        "    'optimizer': ['adam', 'sgd'],\n",
        "    'learning_rate': [0.001, 0.01]\n",
        "}"
      ],
      "metadata": {
        "id": "DvdxqcEnPwjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_evaluate_model(num_layers, num_neurons, batch_size, optimizer, learning_rate):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    for _ in range(num_layers - 1):\n",
        "        model.add(layers.Conv2D(num_neurons, (3, 3), activation='relu'))\n",
        "        model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(num_neurons, activation='relu'))\n",
        "    model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "    opt = None\n",
        "    if optimizer == 'adam':\n",
        "        opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    elif optimizer == 'sgd':\n",
        "        opt = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=opt,\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    history = model.fit(train_images, train_labels, epochs=5, batch_size=batch_size, validation_split=0.1, verbose=0)\n",
        "\n",
        "    test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=0)\n",
        "    return test_acc"
      ],
      "metadata": {
        "id": "9WydfAMkP0i5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "best_acc = 0\n",
        "best_params = {}\n",
        "\n",
        "for params in ParameterGrid(hyperparameter_grid):\n",
        "    acc = train_evaluate_model(**params)\n",
        "    if acc > best_acc:\n",
        "        best_acc = acc\n",
        "        best_params = params\n",
        "    print(f\"Tested: {params}, Accuracy: {acc}\")\n",
        "\n",
        "print(f\"Best Accuracy: {best_acc}, Best Params: {best_params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Bb4qvnPP2ut",
        "outputId": "779e82d1-87e9-4c12-a628-67703ca808a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tested: {'batch_size': 32, 'learning_rate': 0.001, 'num_layers': 2, 'num_neurons': 32, 'optimizer': 'adam'}, Accuracy: 0.9872999787330627\n",
            "Tested: {'batch_size': 32, 'learning_rate': 0.001, 'num_layers': 2, 'num_neurons': 32, 'optimizer': 'sgd'}, Accuracy: 0.9247000217437744\n",
            "Tested: {'batch_size': 32, 'learning_rate': 0.001, 'num_layers': 2, 'num_neurons': 64, 'optimizer': 'adam'}, Accuracy: 0.9883000254631042\n",
            "Tested: {'batch_size': 32, 'learning_rate': 0.001, 'num_layers': 2, 'num_neurons': 64, 'optimizer': 'sgd'}, Accuracy: 0.9290000200271606\n",
            "Tested: {'batch_size': 32, 'learning_rate': 0.001, 'num_layers': 2, 'num_neurons': 128, 'optimizer': 'adam'}, Accuracy: 0.9894000291824341\n",
            "Tested: {'batch_size': 32, 'learning_rate': 0.001, 'num_layers': 2, 'num_neurons': 128, 'optimizer': 'sgd'}, Accuracy: 0.929099977016449\n",
            "Tested: {'batch_size': 32, 'learning_rate': 0.001, 'num_layers': 3, 'num_neurons': 32, 'optimizer': 'adam'}, Accuracy: 0.9825000166893005\n",
            "Tested: {'batch_size': 32, 'learning_rate': 0.001, 'num_layers': 3, 'num_neurons': 32, 'optimizer': 'sgd'}, Accuracy: 0.8277999758720398\n",
            "Tested: {'batch_size': 32, 'learning_rate': 0.001, 'num_layers': 3, 'num_neurons': 64, 'optimizer': 'adam'}, Accuracy: 0.9878000020980835\n",
            "Tested: {'batch_size': 32, 'learning_rate': 0.001, 'num_layers': 3, 'num_neurons': 64, 'optimizer': 'sgd'}, Accuracy: 0.8930000066757202\n",
            "Tested: {'batch_size': 32, 'learning_rate': 0.001, 'num_layers': 3, 'num_neurons': 128, 'optimizer': 'adam'}, Accuracy: 0.9879000186920166\n",
            "Tested: {'batch_size': 32, 'learning_rate': 0.001, 'num_layers': 3, 'num_neurons': 128, 'optimizer': 'sgd'}, Accuracy: 0.9006999731063843\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "One of the dimensions in the output is <= 0 due to downsampling in conv2d_51. Consider increasing the input size. Received input shape [None, 1, 1, 32] which would produce output shape with a zero or negative value in a dimension.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-fe368523a271>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyperparameter_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_evaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_acc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mbest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-e20d7d27c731>\u001b[0m in \u001b[0;36mtrain_evaluate_model\u001b[0;34m(num_layers, num_neurons, batch_size, optimizer, learning_rate)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_layers\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_neurons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/trackable/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py\u001b[0m in \u001b[0;36mcompute_output_shape\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    355\u001b[0m                 \u001b[0;34m\"One of the dimensions in the output is <= 0 \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0;34mf\"due to downsampling in {self.name}. Consider \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: One of the dimensions in the output is <= 0 due to downsampling in conv2d_51. Consider increasing the input size. Received input shape [None, 1, 1, 32] which would produce output shape with a zero or negative value in a dimension."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### In these experiments, it showcases that it is now the architecture or hyperparameters that determines model behaviour, solely the dataset. Thus, while time to train may vary, all models with the same capacity can learn the same functions."
      ],
      "metadata": {
        "id": "EBguMewnPQRn"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "8-EfL3Pg0JhT",
        "9vaEsAKhRuIT",
        "TKYdpVt204-O",
        "nUANFk5Oi4mD",
        "fM2azTtpU68F",
        "OcjaW36BVDMQ",
        "_RtkRcupGXnc",
        "qFlb1mzTHB1T",
        "_KvWnf_XHUeT",
        "PrsGLNw0d9qK",
        "2TrNk_dYw4bD",
        "1eGz2qhrL69y",
        "C56HquXeIkby",
        "MfEE3z4BIpVo",
        "EBguMewnPQRn"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}